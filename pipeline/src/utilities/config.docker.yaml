
env: "DOCKER"


# Credentials
username: "riklscrap1@gmail.com"

# Scraping settings
scraping:
  urls:
    login_url: "https://www.linkedin.com/checkpoint/rm/sign-in-another-account?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin"
    search_url: "https://www.linkedin.com/jobs/search/?keywords={0}&location={1}"
    search_url_past_month: "https://www.linkedin.com/jobs/search/?keywords={0}&location={1}&f_TPR=r2592000"
  no_of_pages: 1  # Number of pages to scrape
  scroll_delay: 1  # Delay (in seconds) between scrolling through jobs
  timeout: 10  # Timeout for waiting for elements (in seconds)
  role: "data analyst"
  search:
    keyword: "data analyst"
    country: "Canada"


# File paths
paths:
  driver_path: "/app/chromedriver/chromedriver"  # Update to reflect the path inside the Docker container
  password_file: "/app/utilities/pass.txt"  # Path to file containing the password inside Docker container
  save_directory: "/data/"  # Directory where pickle files or other data files will be saved


# Logging settings
logging:
  level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
  log_file_scrape: "scraping.log"  # File where logs will be saved
  log_file_preprocess: "preprocess.log"  # File where logs will be saved
  log_file_classifier: "text_classifier.log"# File where logs will be saved for classification
  log_file_load: "data_load.log"# File where logs will be saved for data load
  log_to_console: true  # Enable/Disable logging to the console


# Browser options
browser:
  headless: true  # Whether to run the browser in headless mode
  window_size: "1920x1080"  # Set the browser window size for better visibility in headless mode

# cookies
cookies_file: "/app/utilities/cookies.pkl"

#MongoDb
mongo:
  uri_path: "/app/utilities/mdb.txt"
  database_name: "linkedindb_prod"
  collection_clean: "clean"
  collection_raw: "raw"
  collection_qualified: "qualified"
  test_mode: False

bert:
  classifier:
    model_path: "/app/model/bert/classifier"
    tokenizer_path: "/app/model/bert/classifier/assets/tokenizer"
    max_length: 150
